{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CNTK Word2Vec Part A:  Data Loader\n",
    "\n",
    "In this tutorial, we will learn word embeddings using the Word2Vec model by [Mikolov et al.](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). We will use the [Text8 Corpus](http://mattmahoney.net/dc/textdata) by Matt Mahoney which is cleaned text obtained from English Wikipedia Dump on Mar. 3, 2006.\n",
    "\n",
    "This tutorial is divided into two parts:\n",
    "- Part A: Familiarize with the dataset that will be used later in the tutorial\n",
    "- Part B: We will build our model to learn word embeddings from the Text8 corpus.\n",
    "\n",
    "# Motivation\n",
    "\n",
    "Simply put, word embeddings means giving a vector representation to words. But why would one do so?\n",
    "\n",
    "For applying machine learning (and, in specific, deep learning), it is very important to give a rich representation to our data as per the problem we are tackling. Think of a machine learning algorithm as a child. To make a child learn, we have books with simpler and easy to grasp words with appropriate pictorial representations as well. This make the child learn all the information contained in the data (book) easily. Same is the case with a machine learning algorithm as well.\n",
    "\n",
    "Image and audio processing datasets already work with rich, high dimensional datasets. Images can be represented as vectors of individual raw pixel intensities and audio data as vectors of power spectral density coefficients. So, for audio and image recognition tasks, all the required information is encoded in the dataset itself.\n",
    "\n",
    "But this is not the case with NLP based tasks. NLP systems treat words as discrete atomic units and so simply the word itself cannot capture relationships easily. Text data, in general, is sparse and hence we need more data to successfully train our models.\n",
    "\n",
    "**Vector Space Models** (VSM) represent(embed) words in a continuous vector space where, **semantically similar words map to nearby points**.\n",
    "\n",
    "# Learning Techniques\n",
    "\n",
    "We have two types of methods to learn word embeddings:\n",
    "\n",
    "1. **Count based Methods**: Here, we compute stats of how often some word co-occurs with the neighbor words in a large text corpus (forming co-occurance matrix) and then map these statistics into small, dense vectors for each word. Example: [GloVe Model](http://nlp.stanford.edu/projects/glove)\n",
    "\n",
    "2. **Predictive Methods**: Here, we directly try to predict a word from its neighbors in terms of small, dense embedding vectors (considered as parameters of the model). Example: Word2Vec\n",
    "\n",
    "# Word2Vec\n",
    "\n",
    "Word2Vec is a computationally efficient predictive model. It has two types which can be explained using the example,\n",
    "\n",
    "the quick brown fox jumped over the lazy dog\n",
    "\n",
    "1. **CBOW**: Continuous Bag of Words model, where we predict a target word from source context words. Example: predicting **fox** from **brown** and **jumped**, given the above sentence.\n",
    "\n",
    "2. **Skip Gram**: here, we predict source context words from the target word. Example: predicting **brown** and **jumped** from **fox**, given the above sentence.\n",
    "\n",
    "Here, in this tutorial we will implement the **Skip Gram Model** of **Word2Vec**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the relevant modules to be used later\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange\n",
    "\n",
    "# Initializing globals\n",
    "\n",
    "vocab_size = 4096\n",
    "data = list()\n",
    "dictpickle = 'w2v-dict.pkl'\n",
    "datapickle = 'w2v-data.pkl'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download\n",
    "\n",
    "We will download the data into local machine. The Text8 Corpus is cleaned text from wikipedia and is widely used for training and testing of word embeddings. It is a zip file of ~31MB, when uncompressed, becomes ~100MB. The below code will look for the zip file in the current directory. If not present, then it will download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    url = 'http://mattmahoney.net/dc/'\n",
    "    if not os.path.exists(filename):\n",
    "        print('Downloading Sample Data..')\n",
    "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data\n",
    "\n",
    "Now that the example text corpus has been downloaded, we can read it into memory. Or, if instead of running the example case, you wish to supply your own data file (.txt), then that can also be done using the 'read_data' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Read the file as a list of words\"\"\"\n",
    "    data = list()\n",
    "    with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "        for line in f:\n",
    "            data += line.split()\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_data_zip(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        bdata = f.read(f.namelist()[0]).split()\n",
    "    data = [x.decode() for x in bdata]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Dataset\n",
    "\n",
    "Next, we make up a vocabulary of required size containing the most frequent words in the corpus and intergerize our corpus, by mapping a word to its corresponding index in the vocabulary. If a word is not present, we consider it as a special 'UNK' (unknown) token.\n",
    "\n",
    "Also, we save the vocabulary as a pickle file for later visualizing our learnt embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    global data, vocab_size\n",
    "    \n",
    "    print('Building Dataset..')\n",
    "    \n",
    "    print('Finding the N most common words in the dataset..')\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocab_size - 1))\n",
    "    print('Done')\n",
    "    \n",
    "    dictionary = dict()\n",
    "    \n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    \n",
    "    print('Integerizing the data..')\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    \n",
    "    print('Done')\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    print('Saving Vocabulary..')\n",
    "    with open(dictpickle, 'wb') as handle:\n",
    "        pickle.dump(dictionary, handle)\n",
    "    print('Done')\n",
    "    \n",
    "    print('Saving the processed dataset..')\n",
    "    with open(datapickle, 'wb') as handle:\n",
    "        pickle.dump(data, handle)\n",
    "    print('Done')\n",
    "    \n",
    "    print('Most common words (+UNK)', count[:5])\n",
    "    print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the example data..\n",
      "Found and verified text8.zip\n",
      "Building Dataset..\n",
      "Finding the N most common words in the dataset..\n",
      "Done\n",
      "Integerizing the data..\n",
      "Done\n",
      "Saving Vocabulary..\n",
      "Done\n",
      "Saving the processed dataset..\n",
      "Done\n",
      "Most common words (+UNK) [['UNK', 3061524], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [0, 3082, 12, 6, 195, 2, 3137, 46, 59, 156] ['UNK', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "def process_text(filename):\n",
    "\n",
    "    if filename == 'runexample':\n",
    "        print('Running on the example data..')\n",
    "        filename = maybe_download('text8.zip', 31344016)\n",
    "        words = read_data_zip(filename)\n",
    "    else:\n",
    "        print('Running on the user specified data')\n",
    "        words = read_data(filename)\n",
    "    \n",
    "    build_dataset(words)\n",
    "    \n",
    "# Running on Example Data (i.e. Text8 Corpus)\n",
    "process_text('runexample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
